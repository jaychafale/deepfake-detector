{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9753618-cdc1-4b7a-91c3-185c049d5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, warnings, math\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c152ef5f-6ddf-436c-a1f8-d6bad5745c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost / lightgbm (install lightgbm if available)\n",
    "from xgboost import XGBRegressor\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40631079-6a15-4c24-9d2f-4304c7e12ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Paths & configuration\n",
    "# -----------------------\n",
    "DATA_PATH = Path(\"data/crimedata.csv\")  # your CSV\n",
    "REPORTS_DIR = Path(\"reports\")\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS = -1\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "TARGET_CANDIDATES = [\"ViolentCrimesPerPop\",\"violentcrimesperpop\",\"violentCrimesPerPop\"]\n",
    "ID_LIKE_COLS = [\n",
    "    \"state\",\"county\",\"community\",\"communityname\",\"fold\",\"countyCode\",\"communityCode\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f749f-a970-42a6-8377-b39708d0fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # Replace UCI missing markers with NaN\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].replace({\"?\": np.nan, \"NA\": np.nan, \"na\": np.nan, \"\": np.nan})\n",
    "    # Try numeric conversion where possible\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def guess_target(df: pd.DataFrame) -> str:\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in TARGET_CANDIDATES:\n",
    "        if cand.lower() in lower:\n",
    "            return lower[cand.lower()]\n",
    "    raise ValueError(f\"Target not found. Expected one of: {TARGET_CANDIDATES}\")\n",
    "\n",
    "def split_Xy(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    y = df[target_col].astype(float)\n",
    "    X = df.drop(columns=[target_col])\n",
    "    # drop id-like columns\n",
    "    drop_cols = [c for c in X.columns if c.lower() in [d.lower() for d in ID_LIKE_COLS]]\n",
    "    X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    # numeric only\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = X[num_cols]\n",
    "    return X, y\n",
    "\n",
    "def rmse_cv(estimator, X, y, cv=CV, n_jobs=N_JOBS) -> float:\n",
    "    scores = -cross_val_score(estimator, X, y, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=n_jobs)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def save_table(df: pd.DataFrame, name: str):\n",
    "    path = REPORTS_DIR / name\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def barh_plot(series: pd.Series, title: str, out_name: str, top=25):\n",
    "    s = series.sort_values(ascending=True).tail(top)\n",
    "    plt.figure(figsize=(8, max(4, 0.25*len(s))))\n",
    "    plt.barh(s.index, s.values)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    out = REPORTS_DIR / out_name\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1fbe9-bed9-41be-a913-e890d3cc796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load & prepare data\n",
    "# -----------------------\n",
    "# robust CSV read (handles encoding oddities)\n",
    "try:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "except UnicodeDecodeError:\n",
    "    df_raw = pd.read_csv(DATA_PATH, encoding=\"latin1\")\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "\n",
    "df = clean_dataframe(df_raw)\n",
    "target_col = guess_target(df)\n",
    "\n",
    "# Drop rows with missing target\n",
    "missing_t = df[target_col].isna().sum()\n",
    "if missing_t > 0:\n",
    "    print(f\"Dropping rows with missing target: {missing_t}\")\n",
    "df = df[~df[target_col].isna()].reset_index(drop=True)\n",
    "\n",
    "X, y = split_Xy(df, target_col)\n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
    "\n",
    "# Split for SHAP/permutation efficiency; CV still uses full data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Preprocessors\n",
    "# -----------------------\n",
    "# Trees don't need scaling; linear/SVM/MLP do\n",
    "pre_tree = ColumnTransformer(\n",
    "    transformers=[(\"num\", SimpleImputer(strategy=\"median\"), X.columns)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pre_linear = ColumnTransformer(\n",
    "    transformers=[(\"num\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), X.columns)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ------------------------------------\n",
    "# EXP-1: Global Feature Importance\n",
    "# ------------------------------------\n",
    "# Train RF and XGB on imputed-only data (no scaling)\n",
    "rf = Pipeline(steps=[(\"pre\", pre_tree),\n",
    "                    (\"model\", RandomForestRegressor(\n",
    "                        n_estimators=500, random_state=RANDOM_SEED, n_jobs=N_JOBS\n",
    "                    ))])\n",
    "\n",
    "xgb = Pipeline(steps=[(\"pre\", pre_tree),\n",
    "                     (\"model\", XGBRegressor(\n",
    "                         n_estimators=800, max_depth=6, learning_rate=0.05,\n",
    "                         subsample=0.9, colsample_bytree=0.9,\n",
    "                         reg_lambda=1.0, random_state=RANDOM_SEED,\n",
    "                         n_jobs=N_JOBS, tree_method=\"hist\"\n",
    "                     ))])\n",
    "\n",
    "print(\"\\nTraining RF/XGB for global importance...\")\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Extract aligned importances (need to map back to original columns)\n",
    "rf_imp = pd.Series(rf.named_steps[\"model\"].feature_importances_, index=X.columns, name=\"RF_importance\")\n",
    "xgb_imp = pd.Series(xgb.named_steps[\"model\"].feature_importances_, index=X.columns, name=\"XGB_importance\")\n",
    "\n",
    "# Permutation importance (on a small sample for speed)\n",
    "perm_base = Pipeline(steps=[(\"pre\", pre_tree),\n",
    "                            (\"model\", XGBRegressor(\n",
    "                                n_estimators=600, max_depth=5, learning_rate=0.05,\n",
    "                                subsample=0.9, colsample_bytree=0.9,\n",
    "                                reg_lambda=1.0, random_state=RANDOM_SEED,\n",
    "                                n_jobs=N_JOBS, tree_method=\"hist\"\n",
    "                            ))]).fit(X_train, y_train)\n",
    "\n",
    "perm_sample = min(800, len(X_test))  # speed cap\n",
    "perm = permutation_importance(\n",
    "    perm_base, X_test.iloc[:perm_sample], y_test.iloc[:perm_sample],\n",
    "    n_repeats=5, random_state=RANDOM_SEED, n_jobs=N_JOBS, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns, name=\"Permutation_importance\").clip(lower=0)\n",
    "\n",
    "# Try SHAP (optional, skip if not installed)\n",
    "try:\n",
    "    import shap\n",
    "    shap_model = xgb.named_steps[\"model\"]\n",
    "    explainer = shap.TreeExplainer(shap_model)\n",
    "    # Use preprocessed input for SHAP: imputed-only matrix\n",
    "    X_imputed = pre_tree.fit_transform(X_train)\n",
    "    # sample for speed\n",
    "    idx = np.random.RandomState(RANDOM_SEED).choice(X_imputed.shape[0], size=min(1000, X_imputed.shape[0]), replace=False)\n",
    "    shap_values = explainer.shap_values(X_imputed[idx])\n",
    "    # SHAP per-feature importance = mean(|shap_value|)\n",
    "    shap_imp_vals = np.abs(shap_values).mean(axis=0)\n",
    "    shap_imp = pd.Series(shap_imp_vals, index=X.columns, name=\"SHAP_importance\")\n",
    "    HAS_SHAP = True\n",
    "except Exception as e:\n",
    "    print(f\"SHAP not available or failed: {e}\")\n",
    "    shap_imp = pd.Series(0, index=X.columns, name=\"SHAP_importance\")\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# Combine importance rankings\n",
    "imp_df = pd.concat([rf_imp, xgb_imp, perm_imp, shap_imp], axis=1).fillna(0.0)\n",
    "\n",
    "# Rank normalize each column (higher = more important)\n",
    "rank_df = imp_df.rank(ascending=False, method=\"average\")\n",
    "avg_rank = rank_df.mean(axis=1).sort_values()  # lower rank number = better\n",
    "feature_rank_table = pd.DataFrame({\n",
    "    \"avg_rank\": avg_rank,\n",
    "    \"RF\": rank_df[\"RF_importance\"],\n",
    "    \"XGB\": rank_df[\"XGB_importance\"],\n",
    "    \"Permutation\": rank_df[\"Permutation_importance\"],\n",
    "    \"SHAP\": rank_df[\"SHAP_importance\"] if HAS_SHAP else np.nan\n",
    "}).sort_values(\"avg_rank\")\n",
    "\n",
    "save_table(feature_rank_table.reset_index(names=\"feature\"), \"feature_global_ranking.csv\")\n",
    "barh_plot(imp_df[\"XGB_importance\"], \"XGBoost Feature Importance (Top 25)\", \"xgb_importance_top25.png\", top=25)\n",
    "barh_plot(imp_df[\"RF_importance\"],  \"RandomForest Feature Importance (Top 25)\", \"rf_importance_top25.png\", top=25)\n",
    "barh_plot(perm_imp,                 \"Permutation Importance (Top 25)\", \"perm_importance_top25.png\", top=25)\n",
    "\n",
    "# ------------------------------------\n",
    "# EXP-2: Feature Subset Search (Top-k)\n",
    "# ------------------------------------\n",
    "top_order = feature_rank_table.index.tolist()  # features sorted by avg_rank ascending\n",
    "K_LIST = [3,5,7,10,15,20,30,50,\"all\"]\n",
    "\n",
    "def subset_cols(k):\n",
    "    return X.columns if k == \"all\" else top_order[:k]\n",
    "\n",
    "def tree_cv_for_subset(cols: List[str]) -> Dict[str,float]:\n",
    "    # Use impute-only preprocessor for trees\n",
    "    pre = ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), cols)], remainder=\"drop\")\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=600, random_state=RANDOM_SEED, n_jobs=N_JOBS),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(n_estimators=600, learning_rate=0.05, max_depth=3, random_state=RANDOM_SEED),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=800, max_depth=6, learning_rate=0.05,\n",
    "                                subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "                                random_state=RANDOM_SEED, n_jobs=N_JOBS, tree_method=\"hist\"),\n",
    "    }\n",
    "    out = {}\n",
    "    for name, mdl in models.items():\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"model\", mdl)])\n",
    "        out[name] = rmse_cv(pipe, X[cols], y)\n",
    "    return out\n",
    "\n",
    "subset_rows = []\n",
    "print(\"\\nRunning feature subset CV...\")\n",
    "for k in K_LIST:\n",
    "    cols = subset_cols(k)\n",
    "    scores = tree_cv_for_subset(cols)\n",
    "    row = {\"k\": (len(cols) if k!='all' else len(cols))}\n",
    "    row.update(scores)\n",
    "    subset_rows.append(row)\n",
    "subset_df = pd.DataFrame(subset_rows).sort_values(\"k\")\n",
    "save_table(subset_df, \"feature_subset_cv_trees.csv\")\n",
    "\n",
    "# Plot XGB RMSE vs k\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(subset_df[\"k\"], subset_df[\"XGBoost\"], marker=\"o\")\n",
    "plt.xlabel(\"Number of top features (k)\")\n",
    "plt.ylabel(\"CV RMSE (lower is better)\")\n",
    "plt.title(\"XGBoost CV RMSE vs Feature Count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / \"xgb_rmse_vs_k.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------------------\n",
    "# EXP-3: Model Family Search\n",
    "# ------------------------------------\n",
    "# Two settings: (A) all features, (B) top-10 features\n",
    "top10 = top_order[:10]\n",
    "\n",
    "def models_for_family():\n",
    "    models = {\n",
    "        # Baselines / Linear\n",
    "        \"DummyMean\": DummyRegressor(strategy=\"mean\"),\n",
    "        \"Ridge\": Ridge(alpha=1.0, random_state=RANDOM_SEED),\n",
    "        \"Lasso\": Lasso(alpha=0.0005, random_state=RANDOM_SEED, max_iter=10000),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.0005, l1_ratio=0.2, random_state=RANDOM_SEED, max_iter=10000),\n",
    "\n",
    "        # Tree Ensembles\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=700, random_state=RANDOM_SEED, n_jobs=N_JOBS),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(n_estimators=700, learning_rate=0.05, max_depth=3, random_state=RANDOM_SEED),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=900, max_depth=6, learning_rate=0.05,\n",
    "                                subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "                                random_state=RANDOM_SEED, n_jobs=N_JOBS, tree_method=\"hist\"),\n",
    "    }\n",
    "    if HAS_LGBM:\n",
    "        models[\"LightGBM\"] = LGBMRegressor(\n",
    "            n_estimators=1200, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.9, colsample_bytree=0.9, random_state=RANDOM_SEED, n_jobs=N_JOBS\n",
    "        )\n",
    "    # Nonlinear\n",
    "    models.update({\n",
    "        \"SVR-RBF\": SVR(C=10.0, epsilon=0.05, kernel=\"rbf\"),\n",
    "        \"MLP\": MLPRegressor(hidden_layer_sizes=(128,64), activation=\"relu\",\n",
    "                            alpha=1e-4, learning_rate_init=1e-3,\n",
    "                            random_state=RANDOM_SEED, max_iter=400)\n",
    "    })\n",
    "    return models\n",
    "\n",
    "def preprocessor_for_model(name: str, cols: List[str]):\n",
    "    # Linear/SVR/MLP -> impute+scale; Trees -> impute only\n",
    "    if name in [\"Ridge\",\"Lasso\",\"ElasticNet\",\"SVR-RBF\",\"MLP\"]:\n",
    "        return ColumnTransformer([(\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), cols)], remainder=\"drop\")\n",
    "    else:\n",
    "        return ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), cols)], remainder=\"drop\")\n",
    "\n",
    "def eval_family(cols: List[str], label: str) -> pd.DataFrame:\n",
    "    records = []\n",
    "    models = models_for_family()\n",
    "    for name, mdl in models.items():\n",
    "        pre = preprocessor_for_model(name, cols)\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"model\", mdl)])\n",
    "        rmse = rmse_cv(pipe, X[cols], y)\n",
    "        records.append({\"Setting\": label, \"Model\": name, \"CV_RMSE\": rmse, \"k_features\": len(cols)})\n",
    "        print(f\"[{label}] {name}: CV RMSE={rmse:.4f}\")\n",
    "    out_df = pd.DataFrame(records).sort_values(\"CV_RMSE\")\n",
    "    return out_df\n",
    "\n",
    "print(\"\\nModel family search (ALL features)...\")\n",
    "mf_all = eval_family(list(X.columns), \"AllFeatures\")\n",
    "save_table(mf_all, \"model_family_cv_all.csv\")\n",
    "\n",
    "print(\"\\nModel family search (TOP-10 features)...\")\n",
    "mf_top10 = eval_family(top10, \"Top10Features\")\n",
    "save_table(mf_top10, \"model_family_cv_top10.csv\")\n",
    "\n",
    "# Combined plot for quick takeaways\n",
    "def plot_models(df: pd.DataFrame, title: str, out_name: str):\n",
    "    df = df.sort_values(\"CV_RMSE\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(df[\"Model\"], df[\"CV_RMSE\"])\n",
    "    plt.xlabel(\"CV RMSE (lower is better)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / out_name, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {REPORTS_DIR / out_name}\")\n",
    "\n",
    "plot_models(mf_all,  \"Model Family — All Features\",   \"models_all_features.png\")\n",
    "plot_models(mf_top10,\"Model Family — Top-10 Features\",\"models_top10_features.png\")\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Artifacts saved into ./reports:\")\n",
    "for p in sorted(REPORTS_DIR.glob(\"*\")):\n",
    "    print(\" -\", p.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
